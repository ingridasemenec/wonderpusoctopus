{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COPERNICUS MARINE DATASET - THE NORTH SEA\n",
    "\n",
    "# Feature importance analysis (using XGBoost)\n",
    "\n",
    "https://help.marine.copernicus.eu/en/articles/8283072-copernicus-marine-toolbox-api-subset\n",
    "\n",
    "https://pypi.org/project/copernicusmarine/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copernicusmarine\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "#import shap\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# To avoid warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import Garbage Collector - we will need it a lot here, since we are dealing with huge files and might have memory issues!\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to log in a Copernicus Marine account to access the data.\n",
    "copernicusmarine.login()\n",
    "# Copernicus username and password."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import datasets as pandas dataframes (skip this if you already have merged_df.csv)\n",
    "\n",
    "(Based on Kshitiz's code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORTH_SEA_REGION_lat = [50, 62]\n",
    "NORTH_SEA_REGION_lon = [-6, 12]\n",
    "TIMEFRAME = [\"1997-01-01T00:00:00\", \"2023-01-01T00:00:00\"]\n",
    "DEPTH = [0, 0.5]\n",
    "\n",
    "TIMEFRAMES_SST = [[\"1997-01-01T00:00:00\", \"2002-12-01T00:00:00\"], [\"2002-12-02T00:00:00\", \"2007-12-01T00:00:00\"], [\"2007-12-02T00:00:00\", \"2012-12-01T00:00:00\"],\n",
    "                  [\"2012-12-02T00:00:00\", \"2017-12-01T00:00:00\"], [\"2017-12-02T00:00:00\", \"2023-01-01T00:00:00\"]]\n",
    "\n",
    "SST_COUNTER = 0\n",
    "\n",
    "CURR_DIR = os.getcwd()\n",
    "DATASETS_DIR = CURR_DIR + \"\\\\datasets_csv\"\n",
    "os.makedirs(DATASETS_DIR, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_polish_dataset(dataset_id, variables, output_name, SST_COUNTER):\n",
    "    HAS_DEPTH = {'bathy', 'carbon', 'chlorophyll', 'pisces'}\n",
    "    \n",
    "    if output_name in HAS_DEPTH:\n",
    "        depth_ = DEPTH\n",
    "    else:\n",
    "        depth_ = [None, None]\n",
    "\n",
    "\n",
    "    if (not output_name == 'bathy') and (not output_name == 'sst'):\n",
    "        time_ = TIMEFRAME\n",
    "    elif output_name == 'sst':\n",
    "        time_ = TIMEFRAMES_SST[SST_COUNTER]\n",
    "        output_name = output_name + '_' + str(SST_COUNTER)\n",
    "    else:\n",
    "        time_ = [None, None]\n",
    "\n",
    "    data_request = {\n",
    "    \"dataset_id\" : dataset_id,\n",
    "    \"variables\" : variables,\n",
    "    \"longitude\" : NORTH_SEA_REGION_lon, \n",
    "    \"latitude\" : NORTH_SEA_REGION_lat,\n",
    "    \"time\" : time_,\n",
    "    \"depth\": depth_\n",
    "    } \n",
    "\n",
    "    if not output_name == 'bathy':\n",
    "        df = copernicusmarine.read_dataframe(dataset_id=data_request[\"dataset_id\"],\n",
    "                                        variables=data_request[\"variables\"],\n",
    "                                        minimum_longitude=data_request[\"longitude\"][0],\n",
    "                                        maximum_longitude=data_request[\"longitude\"][1],\n",
    "                                        minimum_latitude=data_request[\"latitude\"][0],\n",
    "                                        maximum_latitude=data_request[\"latitude\"][1],\n",
    "                                        minimum_depth=data_request[\"depth\"][0],\n",
    "                                        maximum_depth=data_request[\"depth\"][1],\n",
    "                                        start_datetime=data_request[\"time\"][0],\n",
    "                                        end_datetime=data_request[\"time\"][1]\n",
    "                                        )\n",
    "    else:\n",
    "         df = copernicusmarine.read_dataframe(dataset_id=data_request[\"dataset_id\"],\n",
    "                                        variables=data_request[\"variables\"],\n",
    "                                        minimum_longitude=data_request[\"longitude\"][0],\n",
    "                                        maximum_longitude=data_request[\"longitude\"][1],\n",
    "                                        minimum_latitude=data_request[\"latitude\"][0],\n",
    "                                        maximum_latitude=data_request[\"latitude\"][1],\n",
    "                                        minimum_depth=data_request[\"depth\"][0],\n",
    "                                        maximum_depth=data_request[\"depth\"][1]\n",
    "                                        )\n",
    "\n",
    "    df = df.dropna()\n",
    "\n",
    "\n",
    "    # function to coarse grain the data and make resolution same as Pisces data\n",
    "    def coarse_grain(df, features, output_name):\n",
    "        \"\"\"\n",
    "            Parameters\n",
    "            ----------\n",
    "            df: pandas dataframe containing the data accessed from copernicus mariner\n",
    "            features: name of the features in the dataframe\n",
    "\n",
    "            Output\n",
    "            ------\n",
    "            a pandas dataframe with feature values for 0.25 deg x 0.25 deg resolution \n",
    "\n",
    "            \"\"\"\n",
    "        \n",
    "        df[\"0_1\"] = df[\"latitude\"].to_numpy() - np.floor(df[\"latitude\"])\n",
    "        conditions = [df[\"0_1\"] < 0.25,\n",
    "                        (df[\"0_1\"] >= 0.25)  & (df[\"0_1\"] < 0.5),\n",
    "                        (df[\"0_1\"] >= 0.5)  & (df[\"0_1\"] < 0.75),\n",
    "                        (df[\"0_1\"] >= 0.75)  & (df[\"0_1\"] < 1)]\n",
    "        outputs = [0, 0.25, 0.5, 0.75]\n",
    "        df['latitude'] = np.floor(df[\"latitude\"]) + np.select(conditions, outputs)\n",
    "        df = df.drop(columns=[\"0_1\"])\n",
    "\n",
    "        df[\"0_1\"] = df[\"longitude\"].to_numpy() - np.floor(df[\"longitude\"])\n",
    "        # if not redefined then conditions is based on latitude\n",
    "        conditions = [df[\"0_1\"] < 0.25,\n",
    "                    (df[\"0_1\"] >= 0.25)  & (df[\"0_1\"] < 0.5),\n",
    "                    (df[\"0_1\"] >= 0.5)  & (df[\"0_1\"] < 0.75),\n",
    "                    (df[\"0_1\"] >= 0.75)  & (df[\"0_1\"] < 1)]\n",
    "        df['longitude'] = np.floor(df[\"longitude\"]) + np.select(conditions, outputs)\n",
    "        df = df.drop(columns=[\"0_1\"])\n",
    "\n",
    "        if not output_name == 'bathy': \n",
    "            return df.groupby([\"time\", \"latitude\", \"longitude\"])[features].mean()\n",
    "        else:\n",
    "            return df.groupby([\"latitude\", \"longitude\"])[features].mean()\n",
    "    \n",
    "    if not output_name == 'pisces':\n",
    "        df = df.reset_index()\n",
    "        if not output_name == 'bathy':\n",
    "            df_cg = coarse_grain(df, df.columns[3:].tolist(), output_name)\n",
    "        else:\n",
    "            df_cg = coarse_grain(df, df.columns[2:].tolist(), output_name)\n",
    "    else:\n",
    "        # remove depth as index\n",
    "        df = df.reset_index(level=[\"depth\"])\n",
    "        # average across the depth for each (time, latitude, longitude)\n",
    "        df_cg = df.reset_index().groupby([\"time\",\"latitude\",\"longitude\"]).mean()\n",
    "        df_cg = df_cg.drop(columns=[\"depth\"])\n",
    "\n",
    "\n",
    "    # remove time and just have month and year\n",
    "    df_cg = df_cg.reset_index()\n",
    "\n",
    "    if not output_name == 'bathy':\n",
    "        # from https://stackoverflow.com/questions/53509168/extract-year-month-and-day-from-datetime64ns-utc-python\n",
    "        datetimes = pd.to_datetime(df_cg['time'])\n",
    "        df_cg['day'] = datetimes.dt.day\n",
    "        df_cg['month'] = datetimes.dt.month\n",
    "        df_cg['year'] = datetimes.dt.year\n",
    "\n",
    "        # remove the time column\n",
    "        df_cg = df_cg.drop(columns=[\"time\"])\n",
    "        df_cg = df_cg.set_index([\"year\",\"month\",\"day\",\"latitude\",\"longitude\"])\n",
    "        df_cg = df_cg.groupby([\"year\",\"month\",\"latitude\",\"longitude\"]).mean()\n",
    "        try:\n",
    "             df_cg = df_cg.drop(columns = [\"day\"])\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "    df_cg.to_csv(f\"{DATASETS_DIR}/{output_name}_049depth.csv\")\n",
    "\n",
    "    del df, df_cg\n",
    "    gc.collect()\n",
    "\n",
    "    return SST_COUNTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bathy_info = { 'dataset_id': 'cmems_mod_glo_phy_my_0.083deg_static',\n",
    "               'variables': ['deptho'],\n",
    "               'output_name' : 'bathy'}\n",
    "\n",
    "carbon_info = { 'dataset_id': 'dataset-carbon-rep-monthly',\n",
    "               'variables': [\"fgco2\", \"omega_ar\", \"omega_ca\", \"ph\", \"spco2\", \"talk\", \"tco2\"],\n",
    "               'output_name' : 'carbon'}\n",
    "\n",
    "chlorophyll_info = { 'dataset_id': 'cmems_obs-oc_glo_bgc-plankton_my_l4-multi-4km_P1M',\n",
    "               'variables': [\"CHL\"],\n",
    "               'output_name' : 'chlorophyll'}\n",
    "\n",
    "pisces_info = { 'dataset_id': 'cmems_mod_glo_bgc_my_0.25deg_P1M-m',\n",
    "               'variables': [\"fe\", \"no3\", \"o2\", \"po4\", \"si\"],\n",
    "               'output_name' : 'pisces'}\n",
    "\n",
    "sssd_info = { 'dataset_id': 'cmems_obs-mob_glo_phy-sss_my_multi_P1M',\n",
    "               'variables': [\"sos\",\"dos\"],\n",
    "               'output_name' : 'sssd'}\n",
    "\n",
    "sst_info = { 'dataset_id': 'METOFFICE-GLO-SST-L4-REP-OBS-SST',\n",
    "               'variables': [\"analysed_sst\"],\n",
    "               'output_name' : 'sst'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [bathy_info, carbon_info, chlorophyll_info, pisces_info, sst_info]\n",
    "dataset_names = [carbon_info, chlorophyll_info, pisces_info, sst_info]\n",
    "# For sssd, use Kshitiz's notebook directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in tqdm(dataset_names):\n",
    "    if not name['output_name'] == 'sst':\n",
    "        get_and_polish_dataset(name['dataset_id'], name['variables'], name['output_name'], SST_COUNTER)\n",
    "    else:\n",
    "        for _ in range(len(TIMEFRAMES_SST)):\n",
    "            get_and_polish_dataset(name['dataset_id'], name['variables'], name['output_name'], SST_COUNTER)\n",
    "            SST_COUNTER += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sst = [pd.read_csv(f\"{DATASETS_DIR}/sst_{i}_049depth.csv\") for i in range(len(TIMEFRAMES_SST))]\n",
    "df_sst = pd.concat(df_sst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sst.to_csv(f\"{DATASETS_DIR}/sst_049depth.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = []\n",
    "for df in os.listdir(DATASETS_DIR):\n",
    "    if not df.startswith('merged'):\n",
    "        if not df.startswith('bathy'):\n",
    "            all_df.append(pd.read_csv(f\"{DATASETS_DIR}/{df}\").set_index([\"year\", \"month\", \"latitude\", \"longitude\"]))\n",
    "        #else:\n",
    "        #    all_df.append(pd.read_csv(f\"{DATASETS_DIR}/{df}\").set_index([\"latitude\", \"longitude\"]))\n",
    "[len(x) for x in all_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some datasets still have an unnamed column corresponding to an indexing column that we need to drop\n",
    "cleaned_all_df = []\n",
    "for df in all_df:\n",
    "    try:\n",
    "        cleaned_all_df.append(df.drop(columns=['Unnamed: 0']))\n",
    "    except:\n",
    "        cleaned_all_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = cleaned_all_df[0]\n",
    "for df in cleaned_all_df[1:]:\n",
    "    merged_df = merged_df.join(df, how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(f\"{DATASETS_DIR}/merged_df_049depth.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cleaned_all_df, all_df, df_sst\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve merged dataframe (if you already had created merged_df.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_DIR = os.getcwd()\n",
    "DATASETS_DIR = CURR_DIR + \"\\\\datasets_csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(f\"{DATASETS_DIR}\\merged_df_049depth.csv\")\n",
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of outliers for chlorophyll values\n",
    "y = merged_df[['CHL']]\n",
    "print(\"Min, max, average, std, 99th percentile values of chlorophyll in the merged dataframe:\", \n",
    "      [y.min(axis=0)['CHL'], y.max(axis=0)['CHL'], y.median(axis=0)['CHL'], y.std()[0], np.percentile(y, 99)])\n",
    "\n",
    "style = {'facecolor': 'none', 'edgecolor': 'C0', 'linewidth': 3}\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(y, bins=100, **style)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of values above the 99th percentile value\n",
    "chl_threshold =  np.percentile(y, 99)\n",
    "merged_df_chl_filtered = merged_df[merged_df[\"CHL\"] < chl_threshold]\n",
    "len(merged_df_chl_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_original = merged_df.copy()\n",
    "merged_df = merged_df_chl_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of outliers for chlorophyll values\n",
    "y = merged_df[['CHL']]\n",
    "print(\"Min, max, average, std, 99th percentile values of chlorophyll in the merged filtered dataframe:\", \n",
    "      [y.min(axis=0)['CHL'], y.max(axis=0)['CHL'], y.median(axis=0)['CHL'], y.std()[0], np.percentile(y, 99)])\n",
    "\n",
    "style = {'facecolor': 'none', 'edgecolor': 'C0', 'linewidth': 3}\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(y, bins=100, **style)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into train, val, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature and target arrays\n",
    "X = merged_df.drop('CHL', axis = 1)\n",
    "y = merged_df[['CHL']]\n",
    "#X = X.drop('time', axis = 1)\n",
    "#X = X.drop('latitude', axis = 1)\n",
    "#X = X.drop('longitude', axis = 1)\n",
    "\n",
    "del merged_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1729)\n",
    "X_train_train, X_train_test, y_train_train, y_train_test = train_test_split(X_train, y_train, random_state = 1729)\n",
    "\n",
    "del X_train, y_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train, val, and test sets\n",
    "X_train_train.to_csv((f\"{DATASETS_DIR}/X_train_train_049depth.csv\"))\n",
    "y_train_train.to_csv((f\"{DATASETS_DIR}/y_train_train_049depth.csv\"))\n",
    "X_train_test.to_csv((f\"{DATASETS_DIR}/X_train_test_049depth.csv\"))\n",
    "y_train_test.to_csv((f\"{DATASETS_DIR}/y_train_test_049depth.csv\"))\n",
    "X_test.to_csv((f\"{DATASETS_DIR}/X_test_049depth.csv\"))\n",
    "y_test.to_csv((f\"{DATASETS_DIR}/y_test_049depth.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewAtlantis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
